{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28616933",
   "metadata": {},
   "source": [
    "# Rapport du projet IARN - Classification Spam/Ham\n",
    "\n",
    "## Réalisé par :\n",
    "\n",
    "AIT AMARA Mohamed, 181831072170\n",
    "\n",
    "BOUROUINA Rania, 181831052716\n",
    "\n",
    "CHIBANE Ilies, 181831072041\n",
    "\n",
    "HAMMAL Ayoub, 181831048403\n",
    "\n",
    "## Outils et librairies\n",
    "\n",
    "Nous utilisons pour dans ce projet les librairies suivantes :\n",
    "\n",
    "- **NLTK** : Librairie de traitement de langague, contient des méthodes de traitement de text (stemming, tokenization, ...). Nous utilisons *SnowballStemmer* pour réduire les tokens en à leurs radicaux.\n",
    "\n",
    "- **Scikit-learn** : Nous utilisons cette librairie pour entrainer des algorithmes d'apprentissage machine comme les SVM et les Naive Bayes. Nous utilisons les metriques proposées par cette librairie pour comparer les résultats obtenus par chaque modèle.\n",
    "\n",
    "- **Pytorch** : Nous avons choisi Pytorch plutôt que Tensorflow car elle permet d'avoir un contrôl plus strict sur l'exécution du code et la boucle d'entrainement, de la propagation avant jusqu'à la mise à jour des paramètres.\n",
    "\n",
    "## Préparation des données\n",
    "\n",
    "Pour faciliter le chargement des données, nous avons séparé les fichiers en deux dossiers (spam et ham).\n",
    "Certain fichiers sont encodés avec un encodage *latin-1*, nous avons commencé par transcoder ces fichiers en *utf-8*.\n",
    "\n",
    "Ensuite, en suivant les recommendations fournies, nous avons construit une fonction de pré-traitement qui applique les transformations nécessaires sur le text de chaque mail (url, email, dollar, html, nombres, tokenisation et stemmisation).\n",
    "\n",
    "### Construction du vocabulaire\n",
    "\n",
    "Nous chargeons l'ensemble des fichiers une première fois pour construire un dictionnaire de vocabulaire.\n",
    "\n",
    "Tous les tokens des différents fichiers sont concaténés dans une seul list, on fait par la suite le compte des occcurences de chaque token et on ne garde que les 5000 tokens les plus fréquents pour simplifier les calculs. On associe à chaque token un entier entre 1 et 5000, le token 0 représente quant-à lui le padding.\n",
    "\n",
    "Après visualisation de l'histogramme résultant de la différence entre les histogrammes de la classe positive et la classe négative, on remarque qu'il y un contraste claire entre l'occurence de certains groupes de mots entre les deux classes; certain mots sont présents dans une classe seulement plus que l'autre. \n",
    "\n",
    "## Implémentation des modèle\n",
    "\n",
    "Dans ce travail, nous avons utilisé 4 modèles différents que nous présntons par la suite. Nous avons aussi testé ces algorithmes avec les deux encodages; vecteur binaire et vecteur de compteurs.\n",
    "\n",
    "### Modèles d'apprentissage profond\n",
    "\n",
    "Nous gardons en historique les meilleurs paramètres des modèles durant l'apprentissage, et nous les restorons à la fin de l'entrainement.\n",
    "\n",
    "#### Modèle avec LSTM\n",
    "\n",
    "Il est constitué de deux couches; une couche recurrente LSTM et une couche de classification sur le dernier état caché du LSTM. Le réseau à un seul output en activation sigmoid qui représente la probabilité d'appartenance à la classe positive (spam).\n",
    "\n",
    "L'entrée du réseau est une séquence de nombres qui représentent cahcun un token.\n",
    "\n",
    "#### Modèle linéaire\n",
    "\n",
    "On utilise cette fois trois couches linéaires complétement connectées. On commence avec un grand nombre de neurones pour la première couche qui diminue progressivement à chauqe couche.\n",
    "\n",
    "On entraine ce modèle avec les deux représentations vectorielles; binaire et à compteurs.\n",
    "\n",
    "### Modèles d'apprentissage machine\n",
    "\n",
    "#### Naive Bayes\n",
    "\n",
    "Pour cette algorithme, nous utilisons la formule multinomiale de calcul de fréquences d'occurences de chaque mot pour les deux classes, qui est plus adaptée pour ce problème que la version gaussienne.\n",
    "\n",
    "On entraine ce modèle avec les deux représentations vectorielles; binaire et à compteurs.\n",
    "\n",
    "#### SVM\n",
    "\n",
    "On utilise un SVM avec un noyau simple noyau linéaire.\n",
    "\n",
    "On entraine ce modèle avec les deux représentations vectorielles; binaire et à compteurs.\n",
    "\n",
    "## Calcul du threshold\n",
    "\n",
    "Pour améliorer l'accuracy de chaque classifieur, nous utilisons la moyenne géométrique entre la spécificité et la sensibilité pour trouver le meilleur threshold de la classification.\n",
    "\n",
    "$$ gmeans = \\sqrt{tpr \\times (1-fpr)} $$\n",
    "\n",
    "On peut utiliser le score F1 pour trouver le threshold.\n",
    "\n",
    "## Analyse des résultats\n",
    "\n",
    "Le tableau suivant contient les résultats des accuracy calculés sur l'ensemble de test pour chaque algorithme en utilisant le threshold calculé avec la moyenne géométrique de la spécificité et la sensibilité :\n",
    "\n",
    "| Modèle   | LSTM        | Linéaire    | Naive Bayes | SVM         |\n",
    "|----------|-------------|-------------|-------------|-------------|\n",
    "| Accuracy | 83.13%      | 97.37%      | 96.50%      | 96.50%      |\n",
    "\n",
    "En utilisant la représentation vectorielle à compteurs nous n'obtenons pas d'améliorations significatives :\n",
    "\n",
    "| Modèle   | Linéaire    | Naive Bayes | SVM         |\n",
    "|----------|-------------|-------------|-------------|\n",
    "| Accuracy | 96.50%      | 96.82%      | 96.28%      |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
